---
title: "Troubleshooting and Resolving Load Balancer Issues with IIS Between Two Azure VMSS"
publishedAt: "2025-01-03"
summary: "This guide documents the troubleshooting and resolution process for a load balancer issue involving two Azure Virtual Machine Scale Sets hosting IIS."
tag: "Load Balancer"
---

Load balancers are critical components in cloud architectures, ensuring high availability and distributing traffic efficiently. However, when issues arise, identifying and resolving the root cause can be complex. This guide documents the troubleshooting and resolution process for a load balancer issue involving two Azure Virtual Machine Scale Sets (VMSS) hosting IIS (Internet Information Services).

This step-by-step guide aims to assist cloud engineers encountering similar challenges.

## Scenario Overview
### Architecture:
- Two Virtual Machine Scale Sets (VMSS):
- VMSS1: Primary application servers running IIS.
- VMSS2: Secondary failover servers, also running IIS.

### Azure Load Balancer (Standard SKU):
- Configured to distribute traffic across instances in VMSS1 and VMSS2.
- Backend pool: Contains all instances from both scale sets.
= Health probe: Monitors IIS availability using an HTTP path.

### Issue:
- Uneven traffic distribution by the load balancer.
- High latency and frequent timeouts reported by users.
- Traffic predominantly routed to VMSS1, with VMSS2 instances intermittently marked unhealthy.

## Step 1: Initial Investigation

## 1. Verify Load Balancer Configuration
Backend Pool: Confirmed that all VMSS instances were correctly associated with the load balancer.

Health Probe:
Protocol: HTTP
Port: 80
Path: /healthcheck
Interval: 5 seconds
Unhealthy Threshold: 2 failed probes

Observation: Configuration appeared correct, but logs revealed repeated health probe failures for VMSS2.

## 2. Inspect IIS on VMSS Instances

1. Logged into instances using Azure Bastion.
2. Tested the /healthcheck endpoint on VMSS1 and VMSS2:
    VMSS1: Responded with 200 OK.
    VMSS2: Intermittent 503 Service Unavailable.
2. Finding: The application pool hosting the /healthcheck endpoint on VMSS2 was failing intermittently.

## 3. Analyze Application Pool Logs
Checked IIS logs (C:\inetpub\logs\LogFiles\W3SVC1) on VMSS2 instances:
Errors indicated that the application pool was not starting correctly during instance initialization.
Root Cause Identified: Application pool misconfiguration on VMSS2 was causing health probe failures.

## Step 2: Resolving the Issue
Fixing the Application Pool
Updated the application pool configuration on VMSS2 to ensure it started automatically:

<CodeBlock
    className="custom-code-block"
    liveEditor
    codeInstances={[
        {
            code: `appcmd set apppool /apppool.name:"DefaultAppPool" /autoStart:true
appcmd start apppool /apppool.name:"DefaultAppPool"`,
            label: "Git Configuration",
            language: "bash",
        },
    ]}
    copyButton
/>

Restarted IIS:

<CodeBlock
    className="custom-code-block"
    liveEditor
    codeInstances={[
        {
            code: `iisreset`,
            label: "Git Configuration",
            language: "bash",
        },
    ]}
    copyButton
/>

Automated the fix for all instances in VMSS2 using an Azure Custom Script Extension. The script ensured consistent configuration across scale set instances:

<CodeBlock
    className="custom-code-block"
    liveEditor
    codeInstances={[
        {
            code: `# Custom Script for IIS Configuration
appcmd set apppool /apppool.name:"DefaultAppPool" /autoStart:true
iisreset`,
            label: "Git Configuration",
            language: "bash",
        },
    ]}
    copyButton
/>

Updating Health Probe Configuration
Changed the health probe path to point to a more reliable endpoint (/default.aspx) instead of the custom /healthcheck endpoint:

<CodeBlock
    className="custom-code-block"
    liveEditor
    codeInstances={[
        {
            code: `Path: /default.aspx
Protocol: HTTP
Port: 80
Interval: 5 seconds
Unhealthy Threshold: 2 failures`,
            label: "Git Configuration",
            language: "bash",
        },
    ]}
    copyButton
/>

- Reason: IIS serves /default.aspx as part of its default configuration, reducing the likelihood of probe failures.

Applied the updated configuration via the Azure Portal and confirmed the changes in the backend pool.

Validating Load Balancer Functionality

### Enabled Azure Monitor for Load Balancer:
- Reviewed metrics for backend pool health and probe success rates.
- Confirmed that VMSS2 instances were consistently marked healthy.

### Monitored traffic distribution:

Verified that the load balancer evenly distributed traffic between VMSS1 and VMSS2.

## Step 3: Post-Resolution Steps
Enhanced Monitoring

### Enabled Azure Monitor Alerts:
- Alert on health probe failures.
- Notify when backend pool imbalance exceeds predefined thresholds.
- Implemented Best Practices

### Automated Configuration Management:

Used Terraform to manage VMSS configurations, ensuring consistency and preventing manual errors.

### Example Terraform snippet for VMSS configuration:

<CodeBlock
    className="custom-code-block"
    liveEditor
    codeInstances={[
        {
            code: `resource "azurerm_virtual_machine_scale_set" "example" {
  name                = "vmss-example"
  ...
  os_profile {
    custom_data = filebase64("${path.module}/init-script.sh")
  }
}`,
            label: "Git Configuration",
            language: "bash",
        },
    ]}
    copyButton
/>

### Proactive Maintenance:

Periodically tested health probe endpoints during maintenance windows.
Regularly reviewed and updated the load balancer and VMSS configurations.
Results

### Restored Load Balancer Functionality:

Traffic was evenly distributed across both VMSS1 and VMSS2.

### Improved Application Performance:

Reduced user-reported latency and timeouts.
Achieved consistent backend pool health probe success rates.

### Prevented Future Issues:

Automated configuration management and enhanced monitoring reduced the risk of similar issues.

## Lessons Learned

### Health Probes Are Critical:

Ensure that health probe endpoints are reliable and consistently available.

### Configuration Consistency:

Use automation tools like Terraform or Azure DevOps to enforce consistent configurations across environments.

### Proactive Monitoring:

Enable detailed logging and monitoring to identify potential issues before they impact production.

### Standardized Troubleshooting Approach:

Follow a systematic layer-by-layer investigation process for quicker resolution.

## Conclusion
Troubleshooting load balancer issues can be challenging, especially in complex cloud environments. By following a structured approach—starting from the load balancer configuration and working down to the application layer—we resolved the issue and restored optimal performance.

This guide provides a roadmap for cloud engineers facing similar challenges. The key takeaway is to leverage automation, maintain consistent configurations, and implement proactive monitoring to prevent and quickly resolve issues.

